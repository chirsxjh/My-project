#### AI相关面试题
### Python的Numpy,SciPy和Pandas,Matplotlib的区别
**Numpy** : 基础的数学计算模块，以矩阵为主,纯数学。
**SciPy**: 基于Numpy，提供方法(函数库)直接计算结果，封装了一些高阶抽象和物理模型。比方说做个傅立叶变换，这是纯数学的，用Numpy；做个滤波器，这属于信号处理模型了，在Scipy    
**Pandas**: 提供了一套名为DataFrame的数据结构，适合统计分析中的表结构,在上层做数据分析, 

* NumPy：N维数组容器
* SciPy：科学计算函数库
* Pandas：表格容器

### Redis数据类型有几种
redis是键值对的数据库，有5中主要数据类型：  
字符串类型（string），散列类型（hash），列表类型（list），集合类型（set），有序集合类型（zset）

### 常见的卷集神经网络模型
1. AlexNet(2012)  
AlexNet由5层卷积层、最大池化层、dropout层和3层全连接层组成，网络用于对1000个类别图像进行分类。  
2. VGG Net
2014年牛津大学学者Karen Simonyan 和 Andrew Zisserman创建了一个新的卷积神经网络模型，19层卷积层，卷积核尺寸为3×3，步长为1，最大池化层尺寸为2×2，步长为2.
3. GoogLeNet
GoogleNet是一个具有22个conv层的CNN网络，是ILSVRC2014中以6.7%的TOP5错误率登顶大赛榜首的CNN模型，VGG Net以7.3%位居其下。据我所知，这是第一篇偏离conv和polling顺序堆叠的CNN架构。而且这篇论文的作者也强调，这个模型需要着重考虑内存和功耗（大量堆叠conv层和polling层容易导致过拟合并且消耗大量的计算机算力和内存）。
4. Microsoft ResNet (2015)
微软亚洲研究院2015年创造了一种令人难以置信的152层深度学习模型，ResNet在ILSVRC2015的错误仅仅为3.6%，已经高于了人类肉眼识别的错误率  

### TENSORFLOW常用函数
![参考链接]('https://www.cnblogs.com/guoyaohua/p/9059605.html')


### 监督学习和无监督学习的区别
答;监督学习只利用标记的样本集进行学习，而无监督学习只利用未标记的样本集。

###  查准率和查全率
假设要识别照片中的狗的，在一些照片中，包含12只狗的照片和一些猫的照片。算法识别出有8只狗。在确定的8只狗中，5只实际上是狗（真阳性TP），而其余的是猫（假阳性FP）。该程序的精度为5/8，而其召回率为5/12。   
精确率：P= TP/TP+FP 机器学习西瓜书里： 也叫查准率

召回率：R= TP/TP+FN 机器学习西瓜书里：	也叫查全率

F1值：精确率和召回率的调和均值。

### 评价一个模型好坏的标准是什么
1. 准确率Accuracy
2. 精确度Precision和召回率Recall
3.  F1 score
4.  混淆矩阵
5. ROC曲线与AUC指标
6. TAR，FRR，FAR


[参考链接](https://www.zhihu.com/search?type=content&q=AI%E8%AF%84%E4%BB%B7%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E5%A5%BD%E5%9D%8F%E7%9A%84%E6%A0%87%E5%87%86%E6%98%AF%E4%BB%80%E4%B9%88)

### 简介svm
SVM就是找到一个超平面,使得距离这个超平面最近的点(支持向量)距离超平面最远.
SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）
（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。
[参考链接](https://blog.csdn.net/Jum_Summer/article/details/80793835)

### 常见的机器学习模型
[参考链接](https://www.jianshu.com/p/ab539e9a7955)
* 朴素贝叶斯：优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练。

缺点：对输入数据的表达形式很敏感（连续数据的处理方式）。

* 决策树：优点：计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征。缺点：容易过拟合（后续出现了随机森林，减小了过拟合现象）。

* 逻辑回归：优点：实现简单，分类时计算量非常小，速度很快，存储资源低。缺点：容易欠拟合，一般准确度不高；只能处理二分类问题（softmax解决多分类），需线性可分。

* 损失函数：![](https://img-blog.csdn.net/20170824141731590)

* KNN：优点：思想简单，理论成熟，既可以用来做分类也可以用来做回归； 可用于非线性分类；训练时间复杂度为O(n)；准确度高，对数据没有假设，对outlier不敏感。缺点：计算量大；样本不平衡时的问题；需要大量的内存；未归一化时影响很大。

* SVM：优点：可用于线性/非线性分类，也可以用于回归；低泛化误差；容易解释；计算复杂度较低。缺点：对参数和核函数的选择比较敏感；原始的SVM只比较擅长处理二分类问题。

### 常见的机器学习问题
#### 什么是机器学习过拟合
所谓过拟合,就是指模型在训练集上的效果很好,在测试集上的预测效果很差  
####  如何避免过拟合问题
    1.  重采样bootstrap

    2.  L1,l2正则化

    3. 决策树的剪枝操作

    4.  交叉验证
#### 什么是机器学习的欠拟合
所谓欠拟合就是模型复杂度低或者数据集太小,对模型数据的拟合程度不高,因此模型在训练集上的效果就不好.  
#### 如何避免欠拟合问题
    1.增加样本的数量

    2.增加样本特征的个数

    3.可以进行特征维度扩展

#### 什么是交叉验证？交叉验证的作用是什么？
交叉验证就是将原始数据集(dataset)划分为两个部分.一部分为训练集用来训练模型,另外一部分作为测试集测试模型效果.  
作用：减少过拟合  
